{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41dea7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get AS rank of all ASes from CAIDA AS rank API\n",
    "# This part of the code is used from https://github.com/bgpkit/pyasrank\n",
    "\n",
    "# MIT License\n",
    "\n",
    "# Copyright (c) 2021 Mingwei Zhang\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "ASRANK_ENDPOINT = \"https://api.asrank.caida.org/v2/graphql\"\n",
    "\n",
    "\n",
    "def ts_to_date_str(ts):\n",
    "    \"\"\"\n",
    "    Convert timestamp to a date. This is used for ASRank API which only takes\n",
    "    date strings with no time as parameters.api\n",
    "    \"\"\"\n",
    "    return datetime.utcfromtimestamp(int(ts)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "class AsRank:\n",
    "    \"\"\"\n",
    "    Utilities for using ASRank services\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_ts=\"\"):\n",
    "        self.data_ts = None\n",
    "\n",
    "        # various caches to avoid duplicate queries\n",
    "        self.cache = None\n",
    "        self.cone_cache = None\n",
    "        self.neighbors_cache = None\n",
    "        self.siblings_cache = None\n",
    "        self.organization_cache = None\n",
    "\n",
    "        self.queries_sent = 0\n",
    "\n",
    "        self.session = None\n",
    "        self._initialize_session()\n",
    "\n",
    "        self.init_cache(max_ts)\n",
    "\n",
    "    def _initialize_session(self):\n",
    "        self.session = requests.Session()\n",
    "        retries = Retry(total=5,\n",
    "                        backoff_factor=1,\n",
    "                        status_forcelist=[500, 502, 503, 504])\n",
    "        self.session.mount(ASRANK_ENDPOINT, HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    def _close_session(self):\n",
    "        if self.session:\n",
    "            self.session.close()\n",
    "\n",
    "    def _send_request(self, query):\n",
    "        \"\"\"\n",
    "        send requests to ASRank endpoint\n",
    "        :param query:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        r = self.session.post(url=ASRANK_ENDPOINT, json={'query': query})\n",
    "        r.raise_for_status()\n",
    "        self.queries_sent += 1\n",
    "        return r\n",
    "\n",
    "    def init_cache(self, ts):\n",
    "        \"\"\"\n",
    "        Initialize the ASRank cache for the timestamp ts\n",
    "        :param ts:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.cache = {}\n",
    "        self.cone_cache = {}\n",
    "        self.neighbors_cache = {}\n",
    "        self.siblings_cache = {}\n",
    "        self.organization_cache = {}\n",
    "        self.queries_sent = 0\n",
    "        if isinstance(ts, int):\n",
    "            ts = ts_to_date_str(ts)\n",
    "\n",
    "        ####\n",
    "        # Try to cache datasets available before the given ts\n",
    "        ####\n",
    "        graphql_query = \"\"\"\n",
    "            {\n",
    "              datasets(dateStart:\"2000-01-01\", dateEnd:\"%s\", sort:\"-date\", first:1){\n",
    "                edges {\n",
    "                  node {\n",
    "                    date\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "        \"\"\" % ts\n",
    "        r = self._send_request(graphql_query)\n",
    "\n",
    "        edges = r.json()['data']['datasets']['edges']\n",
    "        if edges:\n",
    "            self.data_ts = edges[0][\"node\"][\"date\"]\n",
    "            return\n",
    "\n",
    "        # if code reaches here, we have not found any datasets before ts. we should now try to find one after ts.\n",
    "        # this is the best effort results\n",
    "        logging.warning(\"cannot find dataset before date %s, looking for the closest one after it now\" % ts)\n",
    "\n",
    "        graphql_query = \"\"\"\n",
    "            {\n",
    "              datasets(dateStart:\"%s\", sort:\"date\", first:1){\n",
    "                edges {\n",
    "                  node {\n",
    "                    date\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "        \"\"\" % ts\n",
    "        r = self._send_request(graphql_query)\n",
    "        edges = r.json()['data']['datasets']['edges']\n",
    "        if edges:\n",
    "            self.data_ts = edges[0][\"node\"][\"date\"]\n",
    "            logging.warning(\"found closest dataset date to be %s\" % self.data_ts)\n",
    "            return\n",
    "        else:\n",
    "            raise ValueError(\"no datasets from ASRank available to use for tagging\")\n",
    "\n",
    "    def _query_asrank_for_asns(self, asns, chunk_size=100):\n",
    "        asns = [str(asn) for asn in asns]\n",
    "        asns_needed = [asn for asn in asns if asn not in self.cache]\n",
    "        if not asns_needed:\n",
    "            return\n",
    "\n",
    "        # https://stackoverflow.com/a/312464/768793\n",
    "        def chunks(lst, n):\n",
    "            \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "            for i in range(0, len(lst), n):\n",
    "                yield lst[i:i + n]\n",
    "\n",
    "        for asns in chunks(asns_needed, chunk_size):\n",
    "\n",
    "            graphql_query = \"\"\"\n",
    "                {\n",
    "                  asns(asns: %s, dateStart: \"%s\", dateEnd: \"%s\", first:%d, sort:\"-date\") {\n",
    "                    edges {\n",
    "                      node {\n",
    "                        date\n",
    "                        asn\n",
    "                        asnName\n",
    "                        rank\n",
    "                        organization{\n",
    "                          country{\n",
    "                            iso\n",
    "                            name\n",
    "                          }\n",
    "                          orgName\n",
    "                          orgId\n",
    "                        } asnDegree {\n",
    "                          provider\n",
    "                          peer\n",
    "                          customer\n",
    "                          total\n",
    "                          transit\n",
    "                          sibling\n",
    "                        }\n",
    "                      }\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "            \"\"\" % (json.dumps(asns), self.data_ts, self.data_ts, len(asns))\n",
    "            r = self._send_request(graphql_query)\n",
    "            try:\n",
    "                for node in r.json()['data']['asns']['edges']:\n",
    "                    data = node['node']\n",
    "                    if data['asn'] not in self.cache:\n",
    "                        if \"asnDegree\" in data:\n",
    "                            degree = data[\"asnDegree\"]\n",
    "                            degree[\"provider\"] = degree[\"provider\"] or 0\n",
    "                            degree[\"customer\"] = degree[\"customer\"] or 0\n",
    "                            degree[\"peer\"] = degree[\"peer\"] or 0\n",
    "                            degree[\"sibling\"] = degree[\"sibling\"] or 0\n",
    "                            data[\"asnDegree\"] = degree\n",
    "                        self.cache[data['asn']] = data\n",
    "                for asn in asns:\n",
    "                    if asn not in self.cache:\n",
    "                        self.cache[asn] = None\n",
    "            except KeyError as e:\n",
    "                logging.error(\"Error in node: {}\".format(r.json()))\n",
    "                logging.error(\"Request: {}\".format(graphql_query))\n",
    "                raise e\n",
    "\n",
    "    ##########\n",
    "    # AS_ORG #\n",
    "    ##########\n",
    "\n",
    "    def are_siblings(self, asn1, asn2):\n",
    "        \"\"\"\n",
    "        Check if two ASes are sibling ASes, i.e. belonging to the same organization\n",
    "        :param asn1: first asn\n",
    "        :param asn2: second asn\n",
    "        :return: True if asn1 and asn2 belongs to the same organization\n",
    "        \"\"\"\n",
    "        self._query_asrank_for_asns([asn1, asn2])\n",
    "        if any([self.cache[asn] is None for asn in [asn1, asn2]]):\n",
    "            return False\n",
    "        try:\n",
    "            return self.cache[asn1][\"organization\"][\"orgId\"] == self.cache[asn2][\"organization\"][\"orgId\"]\n",
    "        except TypeError:\n",
    "            # we have None for some of the values\n",
    "            return False\n",
    "\n",
    "    def get_organization(self, asn):\n",
    "        \"\"\"\n",
    "        Keys:\n",
    "        - country\n",
    "        - orgName\n",
    "        - orgId\n",
    "\n",
    "        Example return value:\n",
    "        {'country': {'iso': 'US', 'name': 'United States'}, 'orgName': 'Google LLC', 'orgId': 'f7b8c6de69'}\n",
    "\n",
    "        :param asn:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._query_asrank_for_asns([asn])\n",
    "        if self.cache[asn] is None:\n",
    "            return None\n",
    "        return self.cache[asn][\"organization\"]\n",
    "\n",
    "    def get_registered_country(self, asn):\n",
    "        \"\"\"\n",
    "        Get ASes registered country, formated in ISO country code. For example: United States -> US.\n",
    "        \"\"\"\n",
    "        self._query_asrank_for_asns([asn])\n",
    "        if self.cache[asn] is None:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            return self.cache[asn][\"organization\"][\"country\"][\"iso\"]\n",
    "        except KeyError:\n",
    "            return None\n",
    "        except TypeError:\n",
    "            return None\n",
    "\n",
    "    ###########\n",
    "    # AS_RANK #\n",
    "    ###########\n",
    "\n",
    "    def get_degree(self, asn):\n",
    "        \"\"\"\n",
    "        Get relationship summary for asn, including number of customers, providers, peers, etc.\n",
    "\n",
    "        Example return dictionary:\n",
    "        {\n",
    "            \"provider\": 0,\n",
    "            \"peer\": 31,\n",
    "            \"customer\": 1355,\n",
    "            \"total\": 1386,\n",
    "            \"transit\": 1318,\n",
    "            \"sibling\": 25\n",
    "        }\n",
    "        :param asn:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._query_asrank_for_asns([asn])\n",
    "        if self.cache[asn] is None:\n",
    "            return None\n",
    "\n",
    "        return self.cache[asn][\"asnDegree\"]\n",
    "\n",
    "    def is_sole_provider(self, asn_pro, asn_cust):\n",
    "        \"\"\"\n",
    "        Verifies if asn_pro and asn_cust are in a customer provider relationship\n",
    "        and asn_pro is the sole upstream of asn_cust (no other providers nor peers\n",
    "        are available to asn_cust).\n",
    "\n",
    "        This function is ported from dataconcierge.ASRank.check_single_upstream. The name of which is confusing, thus\n",
    "        renamed to is_sole_provider.\n",
    "\n",
    "        :param asn_pro: provider ASn (string)\n",
    "        :param asn_cust: ASn in customer cone (string)\n",
    "        :return: True or False\n",
    "        \"\"\"\n",
    "        asn_cust_degree = self.get_degree(asn_cust)\n",
    "        if asn_cust_degree is None:\n",
    "            # missing data for asn_cust\n",
    "            return False\n",
    "        if asn_cust_degree[\"provider\"] == 1 and asn_cust_degree[\"peer\"] == 0 and \\\n",
    "                self.get_relationship(asn_pro, asn_cust) == \"p-c\":\n",
    "            # asn_cust has one provider, no peer, and the provider is asn_pro\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_relationship(self, asn0, asn1):\n",
    "        \"\"\"\n",
    "        Get the AS relationship between asn0 and asn1.\n",
    "\n",
    "        asn0 is asn1's:\n",
    "        - provider: \"p-c\"\n",
    "        - customer: \"c-p\"\n",
    "        - peer: \"p-p\"\n",
    "        - other: None\n",
    "\n",
    "        :param asn0:\n",
    "        :param asn1:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        graphql_query = \"\"\"\n",
    "            {\n",
    "              asnLink(asn0:\"%s\", asn1:\"%s\", date:\"%s\"){\n",
    "              relationship\n",
    "              }\n",
    "            }\n",
    "        \"\"\" % (asn0, asn1, self.data_ts)\n",
    "        r = self._send_request(graphql_query)\n",
    "        if r.json()[\"data\"][\"asnLink\"] is None:\n",
    "            return None\n",
    "        rel = r.json()[\"data\"][\"asnLink\"].get(\"relationship\", \"\")\n",
    "\n",
    "        if rel == \"provider\":\n",
    "            # asn1 is the provider of asn0\n",
    "            return \"c-p\"\n",
    "\n",
    "        if rel == \"customer\":\n",
    "            # asn1 is the customer of asn0\n",
    "            return \"p-c\"\n",
    "\n",
    "        if rel == \"peer\":\n",
    "            # asn1 is the peer of asn0\n",
    "            return \"p-p\"\n",
    "\n",
    "        return None\n",
    "\n",
    "    def in_customer_cone(self, asn0, asn1):\n",
    "        \"\"\"\n",
    "        Check if asn0 is in the customer cone of asn1\n",
    "        :param asn0:\n",
    "        :param asn1:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if asn1 in self.cone_cache:\n",
    "            return asn0 in self.cone_cache[asn1]\n",
    "\n",
    "        graphql_query = \"\"\"\n",
    "        {\n",
    "          asnCone(asn:\"%s\", date:\"%s\"){\n",
    "            asns {\n",
    "              edges {\n",
    "                node {\n",
    "                  asn\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        \"\"\" % (asn1, self.data_ts)\n",
    "        r = self._send_request(graphql_query)\n",
    "        data = r.json()[\"data\"][\"asnCone\"]\n",
    "        if data is None:\n",
    "            return False\n",
    "        asns_in_cone = {node[\"node\"][\"asn\"] for node in data[\"asns\"][\"edges\"]}\n",
    "        self.cone_cache[asn1] = asns_in_cone\n",
    "        return asn0 in asns_in_cone\n",
    "\n",
    "    def cache_asrank_chunk(self, asns: list, chunk_size: int):\n",
    "        \"\"\"\n",
    "        Query asrank info in chunk to boost individual asrank queries performance later.\n",
    "\n",
    "        :param asns:\n",
    "        :param chunk_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._query_asrank_for_asns(asns, chunk_size)\n",
    "\n",
    "    def get_all_siblings_list(self, asns, chunk_size=100):\n",
    "        self._query_asrank_for_asns(asns, chunk_size)\n",
    "        res = {}\n",
    "        for asn in asns:\n",
    "            res[asn] = self.get_all_siblings(asn)\n",
    "        return res\n",
    "\n",
    "    def get_all_siblings(self, asn, skip_asrank_call=False):\n",
    "        \"\"\"\n",
    "        get all siblings for an ASN\n",
    "        :param asn: AS number to query for all siblings\n",
    "        :param skip_asrank_call: skip asrank call if already done\n",
    "        :return: a tuple of (TOTAL_COUNT, ASNs)\n",
    "        \"\"\"\n",
    "        # FIXME: pagination does not work here. Example ASN5313.\n",
    "        asn = str(asn)\n",
    "        if asn in self.siblings_cache:\n",
    "            return self.siblings_cache[asn]\n",
    "\n",
    "        if not skip_asrank_call:\n",
    "            self._query_asrank_for_asns([asn])\n",
    "\n",
    "        if asn not in self.cache or self.cache[asn] is None:\n",
    "            return 0, []\n",
    "        asrank_info = self.cache[asn]\n",
    "        if \"organization\" not in asrank_info or asrank_info[\"organization\"] is None:\n",
    "            return 0, []\n",
    "\n",
    "        org_id = self.cache[asn][\"organization\"][\"orgId\"]\n",
    "\n",
    "        if org_id in self.organization_cache:\n",
    "            data = self.organization_cache[org_id]\n",
    "        else:\n",
    "            graphql_query = \"\"\"\n",
    "            {\n",
    "            organization(orgId:\"%s\"){\n",
    "              orgId,\n",
    "              orgName,\n",
    "              members{\n",
    "                numberAsns,\n",
    "                numberAsnsSeen,\n",
    "                asns{totalCount,edges{node{asn,asnName}}}\n",
    "              }\n",
    "            }}        \n",
    "            \"\"\" % org_id\n",
    "            r = self._send_request(graphql_query)\n",
    "            data = r.json()[\"data\"][\"organization\"]\n",
    "            self.organization_cache[org_id] = data\n",
    "\n",
    "        if data is None:\n",
    "            return 0, []\n",
    "\n",
    "        total_cnt = data[\"members\"][\"asns\"][\"totalCount\"]\n",
    "        siblings = set()\n",
    "        for sibling_data in data[\"members\"][\"asns\"][\"edges\"]:\n",
    "            siblings.add(sibling_data[\"node\"][\"asn\"])\n",
    "        if asn in siblings:\n",
    "            siblings.remove(asn)\n",
    "            total_cnt -= 1\n",
    "\n",
    "        # NOTE: this assert can be wrong when number of siblings needs pagination\n",
    "        # assert len(siblings) == total_cnt - 1\n",
    "\n",
    "        siblings = list(siblings)\n",
    "        self.neighbors_cache[asn] = (total_cnt, siblings)\n",
    "        return total_cnt, siblings\n",
    "\n",
    "    def get_neighbor_ases(self, asn):\n",
    "        if asn in self.neighbors_cache:\n",
    "            return self.neighbors_cache[asn]\n",
    "\n",
    "        res = {\"providers\": [], \"customers\": [], \"peers\": []}\n",
    "        graphql_query = \"\"\"\n",
    "        {\n",
    "          asn(asn: \"%s\") {\n",
    "            asn\n",
    "            asnLinks {\n",
    "              edges {\n",
    "                node {\n",
    "                  asn1 {\n",
    "                    asn\n",
    "                  }\n",
    "                  relationship\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        \"\"\" % asn\n",
    "        r = self._send_request(graphql_query)\n",
    "\n",
    "        data = r.json()[\"data\"][\"asn\"]\n",
    "        if data is None:\n",
    "            return res\n",
    "        for neighbor in data[\"asnLinks\"][\"edges\"]:\n",
    "            neighbor_asn = neighbor[\"node\"][\"asn1\"][\"asn\"]\n",
    "            neighbor_rel = neighbor[\"node\"][\"relationship\"]\n",
    "            res[\"{}s\".format(neighbor_rel)].append(neighbor_asn)\n",
    "        self.neighbors_cache[asn] = res\n",
    "        return res\n",
    "\n",
    "    def get_asrank_for_asns(self, asn_lst):\n",
    "        \"\"\"\n",
    "        retrieve ASRank data for asns.\n",
    "        :param asn_lst:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        asn_lst = [str(asn) for asn in asn_lst]\n",
    "        self._query_asrank_for_asns(asn_lst)\n",
    "\n",
    "        res = {}\n",
    "        for asn in asn_lst:\n",
    "            res[asn] = self.cache.get(asn, None)\n",
    "        return res   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5561fa69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': {'iso': 'CN', 'name': 'China'},\n",
       " 'orgName': 'China Internet Network Infomation Center',\n",
       " 'orgId': '2db383e082'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find relationship between ASes on an AS Path\n",
    "as_rank = AsRank()\n",
    "as_rank.get_organization(\"24409\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d98088ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the ASNs represented into different columns as a single string\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "df = pd.read_csv('/home/shyam/phd-work/datasets/critical_infra_nl/path_ms_abn_02_14_03_14_cutoff_05_collector_limit_routviews.csv', header = None)\n",
    "\n",
    "# Merge all the ASNs represented into different columns as a single string\n",
    "df['as_path'] = df[df.columns[0:]].apply(\n",
    "    lambda x: ','.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    "    )\n",
    "# Save it into a new file\n",
    "df.to_csv('/home/shyam/phd-work/datasets/critical_infra_nl/valley_free_check/path_ms_abn_02_14_03_14_cutoff_05_collector_limit2_routeviews.csv')\n",
    "\n",
    "#CAUTION: Sometimes the output file contains ASN with .0 at the end which has to be removed manually in excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28652758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the number of hops \n",
    "import pandas as pd\n",
    "df = pd.read_csv('/home/shyam/phd-work/datasets/critical_infra_nl/valley_free_check/path_ms_abn_02_14_03_14_cutoff_05_collector_limit2_routeviews.csv')\n",
    "df_asn_number = [] # List to store number of asns\n",
    "df_relationships = [] #List to store relationships between two ASNs\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    asn_string = row['as_path']\n",
    "    # Get number of asns fromt the string separated by comma\n",
    "    as_path = asn_string.split(\",\")\n",
    "    no_of_asns = asn_string.count(\",\") -1\n",
    "    df_asn_number.append(no_of_asns)\n",
    "    \n",
    "    path_len = len(as_path)\n",
    "    r = []\n",
    "    for k,asn in enumerate(as_path):\n",
    "        if (k+1) is not path_len:\n",
    "            r.append(as_rank.get_relationship(as_path[k],as_path[k+1]))\n",
    "    df_relationships.append(r)\n",
    "df[\"no_of_hops\"] =  df_asn_number\n",
    "\n",
    "df[\"relationships\"] =  df_relationships\n",
    "\n",
    "df.to_csv('/home/shyam/phd-work/datasets/critical_infra_nl/valley_free_check/path_ms_abn_02_14_03_14_cutoff_05_collector_limit2_routeviews.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(40) # Get top 40 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee361ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check valley-free condition in AS_Path: \n",
    "# Conditions to avoid valley \n",
    "#a) 'p-c' is followed by 'c-p'.  validity\n",
    "#b) There exists at least a \"None\" element. \n",
    "#c) Number of 'p-p' is more than 1. \n",
    "#d) 'p-p' is followed by 'c-p'\n",
    "\n",
    "def is_valley_free(lst):\n",
    "    flag = True\n",
    "    if \"None\" in lst:\n",
    "        flag = False\n",
    "\n",
    "    if op.countOf(lst, \"'p-p'\") > 1:\n",
    "        flag = False \n",
    "\n",
    "    for i in range(len(lst) - 1):\n",
    "        if lst[i] == \"'p-c'\":\n",
    "            if lst[i+1] == \"'c-p'\" or lst[i+1] == \"'p-p'\":\n",
    "                flag = False\n",
    "        elif lst[i] == \"'p-p'\":\n",
    "            if lst[i+1] == \"'c-p'\":\n",
    "                flag = False\n",
    "    return flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a4c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditions to check valley free\n",
    "\"\"\"\n",
    "1. If there are multiple P2P paths, it is invalid. 0-0 is invalid\n",
    "2. A P2C connection cannot be followed by a P2P connection. -1->0, -1->+1 are invalids while -1->-1 is valid\n",
    "3. A P2P connection cannot be followed by a C2P connection. Sequence like 0->+1 is invalid. 0-> -1 is valid.\n",
    "\"\"\"\n",
    "\n",
    "# Ignore those rows which relationship has value None\n",
    "import pandas as pd\n",
    "import operator as op\n",
    "\n",
    "df = pd.read_csv('/home/shyam/phd-work/datasets/critical_infra_nl/valley_free_check/path_ms_abn_02_14_03_14_cutoff_05_collector_limit2_routeviews.csv')\n",
    "path_valid = [] #List to store validity status of the overall path\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    validity = 1\n",
    "    rel_string = row['relationships']\n",
    "    # Converting string to list\n",
    "    rel_list = rel_string.strip('][').split(', ')\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # ASes without any relationship are considered as invalid AS path\n",
    "    if \"None\" in rel_list:\n",
    "        validity = 0\n",
    "        \n",
    "    # Multiple p-p relationship can not exist    \n",
    "    elif op.countOf(rel_list, \"'p-p'\") > 1:\n",
    "        validity = 0\n",
    "    else:\n",
    "        validity = 1\n",
    "        path_valid.append(validity)\n",
    "   \"\"\"\n",
    "    validity = is_valley_free(rel_list)\n",
    "    path_valid.append(validity)\n",
    "df[\"validity\"] =  path_valid\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('/home/shyam/phd-work/datasets/critical_infra_nl/valley_free_check/path_ms_abn_02_14_03_14_cutoff_05_collector_limit2_routeviews.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afca2eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of valid paths\n",
    "#df.to_csv('/home/shyam/phd-work/datasets/critical_infra_nl/valley_free_check/path_ms_ing_02_14_18_cutoff_03_collector_limit3.csv')\n",
    "df2 = df.loc[df['validity'] == True]\n",
    "df2.to_csv('/home/shyam/phd-work/datasets/critical_infra_nl/valley_free_check/path_ms_abn_02_14_03_14_cutoff_05_collector_limit4_routeviews.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672e74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de7c3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ROV of ASNs from rov_overview.json file\n",
    "def get_rov(asn):\n",
    "    import json\n",
    "    rov_score = []\n",
    "    with open('/home/shyam/phd-work/datasets/critical_infra_nl/rovista_14March.json', 'r') as myfile:\n",
    "        data=myfile.read()\n",
    "\n",
    "    # parse file\n",
    "    obj = json.loads(data)\n",
    "\n",
    "    #print(as_only)\n",
    "\n",
    "    rov_score = \"NA\"\n",
    "\n",
    "    for value in obj[\"data\"]:\n",
    "        if value['asn'] == asn:\n",
    "            rov_score = value['ratio']*100\n",
    "            break\n",
    "    return str(rov_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe2945db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the number of hops \n",
    "import pandas as pd\n",
    "df = pd.read_csv('/home/shyam/phd-work/datasets/critical_infra_nl/valley_free_check/path_ms_abn_02_14_03_14_cutoff_05_collector_limit4_routeviews.csv')\n",
    "df_rov = [] #List to store rov of each ASNs\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    asn_string = row['as_path']\n",
    "    # Get number of asns fromt the string separated by comma\n",
    "    as_path = asn_string.split(\",\")\n",
    "   \n",
    "    # Loop and find rov for each asn.\n",
    "    rov = []\n",
    "    for asn in as_path:\n",
    "        rov.append(get_rov(int(asn)))    \n",
    "    df_rov.append(rov)\n",
    "df[\"rov_of_each_asn\"] =  df_rov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c720d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/home/shyam/phd-work/datasets/critical_infra_nl/valley_free_check/path_ms_abn_02_14_03_14_cutoff_05_collector_limit4_routeviews.csv\")\n",
    "# CAUTION: Need to replace manually: rov_of_each_asn column values string ('') with null to make them float for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f99dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuate the ROV of the overall path based on immediate peer's ROV value\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/home/shyam/phd-work/datasets/critical_infra_nl/valley_free_check/path_ms_abn_02_14_03_14_cutoff_05_collector_limit4_routeviews.csv')\n",
    "\n",
    "path_rov = [] #List to store rov of the overall path\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    rov = 0\n",
    "    rov_string = row['rov_of_each_asn']\n",
    "    # Converting string to list\n",
    "    rov_list = rov_string.strip('][').split(', ')\n",
    "    for i in range(1,len(rov_list)):\n",
    "        if rov_list[i] == \"0.0\":\n",
    "            avg_rov = 0\n",
    "            break\n",
    "        elif rov_list[i] == \"NA\":\n",
    "            avg_rov = \"NA\"\n",
    "            break\n",
    "        else:\n",
    "            # Convert string to float representation\n",
    "            rov_list[i] = rov_list[i].replace(\"'\",\"\" )\n",
    "            rov += int(float(rov_list[i]))\n",
    "            avg_rov = rov/i \n",
    "    path_rov.append(avg_rov)\n",
    "   \n",
    "df[\"rov_path\"] =  path_rov\n",
    "\n",
    "df.to_csv(\"/home/shyam/phd-work/datasets/critical_infra_nl/valley_free_check/path_ms_abn_02_14_03_14_cutoff_05_collector_limit4_routeviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302578e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41164ae0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/shyam/phd-work/datasets/critical_infra_nl/valley_free_check/path_ms_vitens_02_14_03_14_cutoff_03_collector_limit4_routeviews.csv')\n",
    "len(df.loc[(df['rov_path'] >= 99)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e059beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/shyam/phd-work/datasets/critical_infra_nl/valley_free_check/path_ms_ing_02_14_03_14_cutoff_04_collector_limit4_routeviews.csv')\n",
    "df2 = df.loc[(df['rov_path'] >= 99)]\n",
    "\n",
    "df2[\"as_path\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
